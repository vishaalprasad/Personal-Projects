What is Metropolis-Hastings?
NB: I assume the reader is familiar with both Markov Chains and Monte Carlo methods.

Let f be the probability function we wish to sample from.

Metropolis-Hastings is an MCMC (Markov Chain Monte Carlo) method for sampling from a dataset. It is worth learning how Monte Carlo methods work as a primer, because they serve as a useful contrast to illustrate the use case of M-H. In particular, MCMC methods (including M-H) as a whole are useful when the pdf can't be directly sampled from and IID samples can't be directly generated. However, other methods (see: rejection sampling) can be used here, as well. Where MCMC methods shine is in high-dimensional space.

. MCMC methods take autocorrelated samples instead of IID samples, and require only a distribution proportional to the one you wish to model. This makes them usable in a wide array of problem types. The downside is their very strength: the autocorrelation requirement means that the model may take time to sample the distribution (it has weaker guarantees, so the asymptotic guarantees require more examples). That is why they're used when other methods can't work or don't work efficiently.

MCMC algorithms, as the name suggests, work their magic through the "Markov Chain" component. In essence, Metropolis-Hastings constructs a certain type of transition matrix (i.e. of a Markov Chain) to transition from one sample to the next. This matrix is constructed in such a way that the limiting distribution of the Markov Chain is the very probability distribution f we wish to model. In order to construct such a transition matrix, we can choose any Markov Chain Q with a special property called ergodicity. Q proposes samples. Note this is why M-H is inherently auto-correlated. Q's proposal is then modified by a certain term called the acceptance probability. When the full set of possibilities are considered, we have a new transition matrix. We want this new matrix to have the limiting distribution of our function f, and that can be done by specifically choosing the acceptance probability, which comes off to be something intuitive.

That is a high level explanation of what M-H is all about! The code in this folder is simply a toy example of attempting to model a standard Gaussian using M-H. The code is pretty easily extensible to generalize for various functions, but for didactic purposes, this should suffice.
